

 %This idea was influenced by the \emph{"Fat Thumb: Using the Thumb's Contact Size for Single-Handed Mobile Interaction"} \cite{Boring2012} which deals with a very similar problem. It presents \emph{Fat Thumb} as an alternative technique that adds a dimension to touch input on a mobile device, thus giving different meanings to touch motion. It was done by making use of thumb's contact size allowing seamless mode switching. Testing supported the hypothesis that \emph{Fat Thumb} is at least as fast as other related techniques.

%FingerSkate ergonomic based study \cite{Son:2013:FMM:2508468.2514733}, introduces a variation to the current multi-touch operations. It aims to make this operations less constrained and more continuous. With FingerSkate, once one starts a multi-touch operation, one can continue the operation without having to maintain both fingers on the screen. It also addresses to major ergonomic issues, that simple multi-touch operations have.
%Thumb Rock \cite{Bonnet2013} is another approach in interacting with mobile devices taking advantage of the contact size. In particular it presents an in-drag gesture that consists of rolling the thumb back and forth on a touch surface. Taking advantage of this, many operations on mobile devices are simplified. It can be used as a supplement to tapping, allowing editing or zooming depending on the application. 

%\emph{"Contact size as an input parameter is closely related to pressure (i.e., more pressure suggests a larger contact size due to flattening of the finger"} \textbf{\cite{Boring2012}}. Ramos et al. \cite{Ramos2004} found that only a level of six different pressure values is actually optimal and distinguishable by the users. However his study has been done using a stylus as input to a tablet device. In GraspZoom [5] a different approach was chosen integrating pressure in the interaction with the mobile device. Users could press the back of the device, allowing them to temporally switch modes (e.g. from panning to zooming). All the above systems are investigating the different ways of pressure as an input parameter, but none of these systems explore the use of simulated pressure --- finger's contact size --- while simultaneously moving the finger, on a tablet device.

%Those results motivated and inspired me on working and expanding this technique in the tablet market, trying to achieve significant improvements, utilizing a finger's contact size. Our ultimate target is to develop new ways of interaction with the iPad, which will enhance the current ones. We have to deal with the finger's contact size area and more specifically, with how much detail we can obtain from this area, to give us the ability of mapping many different actions to different contact sizes. 



% \section{After Fat Thumb}

% \textbf{PointPose: finger pose estimation for touch input on mobile devices using a depth sensor \cite{Kratz:2013:PFP:2512349.2512824}}




% The expressiveness of touch input can be increased by detecting additional finger pose information at the point of touch such as finger rotation and tilt. PointPose is a prototype that performs finger pose estimation at the location of touch using a short-range depth sensor viewing the touch screen of a mobile device. We present an algorithm that extracts finger rotation and tilt from a point cloud generated by a depth sensor oriented towards the device's touchscreen. The results of two user studies we conducted show that finger pose information can be extracted reliably using our proposed method. We show this for controlling rotation and tilt axes separately and also for combined input tasks using both axes. With the exception of the depth sensor, which is mounted directly on the mobile device, our approach does not require complex external tracking hardware, and, furthermore, external computation is unnecessary as the finger pose extraction algorithm can run directly on the mobile device. This makes PointPose ideal for prototyping and developing novel mobile user interfaces that use finger pose estimation.


% \textbf{
% Pseudo-pressure detection and its use in predictive text entry on touchscreens
% \cite{Arif:2013:PDU:2541016.2541024}}

% In this article we first present a new hybrid technique that combines existing time- and touch-point-based approaches to simulate pressure detection on standard touchscreens. Results of two user studies show that the new hybrid technique can distinguish (at least) two pressure levels, where the first requires on average 1.04 N and the second 3.24 N force on the surface. Then, we present a novel pressure-based predictive text entry technique that utilizes our hybrid pressure detection to enable users to bypass incorrect predictions by applying extra pressure on the next key. For inputting short English phrases with 10\% non-dictionary words a comparison with conventional text entry in a study showed that the new technique increases entry speed by 9\% and decreases error rates by 25\%. Also, most users (83\%) favour the new technique.


% \textbf{ Pinch-drag-flick vs. spatial input: rethinking zoom \& pan on mobile displays  !!! important  \cite{Spindler:2014:PVS:2556288.2557028}}

% The multi-touch-based pinch to zoom, drag and flick to pan metaphor has gained wide popularity on mobile displays, where it is the paradigm of choice for navigating 2D documents. But is finger-based navigation really the gold standard' In this paper, we present a comprehensive user study with 40 participants, in which we systematically compared the Pinch-Drag-Flick approach with a technique that relies on spatial manipulation, such as lifting a display up/down to zoom. While we solely considered known techniques, we put considerable effort in implementing both input strategies on popular consumer hardware (iPhone, iPad). Our results show that spatial manipulation can significantly outperform traditional Pinch-Drag-Flick. Given the carefully optimized prototypes, we are confident to have found strong arguments that future generations of mobile devices could rely much more on spatial interaction principles.

% \textbf{VibPress: estimating pressure input using vibration absorption on mobile devices  \cite{Hwang:2013:VEP:2493190.2493193}}

% This paper introduces VibPress, a software technique that enables pressure input interaction on mobile devices by measuring the level of vibration absorption with the built-in accelerometer when the device is in contact with a damping surface (e.g., user's hands). This is achieved using a real-time estimation algorithm running on the device. Through a user evaluation, we provide evidence that this system is faster than previous software-based approaches, and accurate as hardware-augmented approaches (up to 99.7\% accuracy). With this work, we also provide an insight about the maximum number of pressure levels that users can reliably distinguish, reporting usability metrics (time, errors and cognitive load) for different pressure levels and types of gripping gestures (press and squeeze).

% \textbf{ FingerSkate: making multi-touch operations less constrained and more continuous \cite{Son:2013:FMM:2508468.2514733}}

% Multi-touch operations are sometimes difficult to perform due to musculoskeletal constraints. We propose FingerSkate, a variation to the current multi-touch operations to make them less constrained and more continuous. With FingerSkate, once one starts a multi-touch operation, one can continue the operation without having to maintain both fingers on the screen. In a pilot study, we observe that participants could learn to FingerSkate easily and were utilizing the new technique actively.

% \textbf{Evaluation of hybrid front- and back-of-device interaction on mobile devices  \cite{Lochtefeld:2013:EHF:2541831.2541865}}

% With the recent trend of increasing display sizes of mobile devices, one-handed interaction has become increasingly difficult when a user wants to maintain a safe grip around the device at the same time. In this paper we evaluate how a combination of hybrid front- and back-of-device touch input can be used to overcome the difficulties when using a mobile device with one hand. Our evaluation shows that, even though such a technique is slower than conventional front-of-device input, it allows for accurate and safe input.

% \textbf{
% Pressure detection on mobile phone by camera and flash  \cite{Low:2014:PDM:2582051.2582062}}

% This paper proposes a method to detect pressure asserted on a mobile phone by utilizing the back camera and flash on the phone. There is a gap between the palm and camera when the phone is placed on the palm. This allows the light from the flashlight to be reflected to the camera. However, when pressure is applied on the phone, the gap will reduce, reducing the brightness captured by the camera. This phenomenon is applied to detect two gestures: pressure applied on the screen and pressure applied when user squeezes the phone. We also conducted an experiment to detect the change in brightness level depending on the amount of force asserted on the phone when it is placed in two positions: parallel to the palm and perpendicular to the palm. The results show that when the force increases, the brightness level decreases. Using the phones ability to detect fluctuations in brightness, various pressure interaction applications such as for gaming purposes may be developed.

% \textbf{---
% Eyes-free text entry interface based on contact area for people with visual impairment  \cite{Goh:2014:ETE:2658779.2658780}}
% We developed an eyes-free text entry interface using contact area to determine pressed state for mobile device with touchscreen. The interface gives audio feedback for a touched character similar to VoiceOver of iPhone, but audio feedbacks of two simultaneous touches are considered. A desired character is entered by pressing once. Independent entry of two fingers can reduce movement distance for searching a character. Whole interaction occurs in touched states, additional tactile feedback can be augmented.


% \textbf{ TouchShield: a virtual control for stable grip of a smartphone using the thumb  \cite{Hong:2013:TVC:2468356.2468589}}

% People commonly manipulate their smartphones using the thumb, but this is often done with an unstable grip in which the phone lays on their fingers, while the thumb hovers over the touch screen. In order to offer a secure and stable grip, we designed a virtual control called TouchShield, which provides place in which the thumb can pin the phone down in order to provide a stable grip. In a user study, we confirmed that this form of control does not interfere with existing touch screen operations, and the possibility that TouchShield can make more stable grip. An incidental function of TouchShield is that it provides shortcuts to frequently used commands via the thumb, a function that was also shown to be effective in the user study.


% \textbf{BezelCursor: bezel-initiated cursor for one-handed target acquisition on mobile touch screens
%   \cite{Li:2013:BBC:2543651.2543680}}

%   Unimanual interaction allows the user to operate the mobile device in a distracted, multitasking scenario and frees the other hand for tasks like carrying a bag, writing a relevant note etc. In such scenarios, the thumb of the hand holding the device is normally the only available finger for touch input [Boring et al. 2012]. However, mainly due to biomechanical limitations of the thumb, only a subregion of the touch screen is comfortable to access by the thumb [Karlson and Bederson 2007], causing awkward hand postures to reach the rest of the screen. This problem of limited screen accessibility by the thumb deteriorates with screens of increasingly bigger sizes, which, however, are getting more and more popular [Fingas 2012].


% \textbf{ 
% Extending the vocabulary of touch events with ThumbRock  \cite{Bonnet:2013:EVT:2532129.2532166}}

% Compared with mouse-based interaction on a desktop interface, touch-based interaction on a mobile device is quite limited: most applications only support tapping and dragging to perform simple gestures. Finger rolling provides an alternative to tapping but uses a recognition process that relies on either per-user calibration, explicit delimiters or extra hardware, making it difficult to integrate into current touch-based mobile devices. This paper introduces ThumbRock, a ready-to-use micro gesture that consists in rolling the thumb back and forth on the touchscreen. Our algorithm recognizes ThumbRocks with more than 96\% accuracy without calibration nor explicit delimiter by analyzing the data provided by the touch screen with a low computational cost. The full trace of the gesture is analyzed incrementally to ensure compatibility with other events and to support real-time feedback. This also makes it possible to create a continuous control space as we illustrate with our MicroSlider, a 1D slider manipulated with thumb rolling gestures.


% \textbf{ Ta-Tap: consecutive distant tap operations for one-handed touch screen use  \cite{Heo:2013:TCD:2508468.2514725}}

% Tapping on the same point twice is a common operation known as double tap, but tapping on distant points in sequence is underutilized. In this poster we explore the potential uses of consecutive distant tap operations, which we call Ta-Tap. As a single-touch operation, it is expected to be particularly useful for single-handed touch screen use. We examined three possible uses of Ta-Tap: simulating multi-touch operations, invoking a virtual scroll wheel, and invoking a pie-menu. We verified the feasibility of Ta-Tap through the experiment.


% \textbf{  \cite{}}
% \textbf{  \cite{}}
% \textbf{  \cite{}}

% \textbf{  \cite{}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{B Pressure}
% BASED ON FAT THUMB

% Pressure as input parameter is closely related to the contact size (i.e., more pressure suggests a larger contact size due to flattening of the finger).


% \textbf{GraspZoom: zooming and scrolling control model for single-handed mobile interaction  \cite{Miyaki:2009:GZS:1613858.1613872}}

% A pressure sensing based single-handed interaction model is presented in this paper. Unlike traditional desktop GUI model, mobile UI model has not been established yet. For example, Apple iPhone proposed "Pinch" operation, which use two fingers to zoom-in and zoom-out objects. However, in a today's hand-held situation, manipulation methods using two fingers are not always good solution because they require two hands in order to hold the device itself in most cases. We propose a single-handed UI scheme "GraspZoom": multi-state input model using pressure sensing. Force Sensitive Resistor (FSR) attached on backside of a mobile phone was employed in order to evaluate effectiveness of pressure based control model. We also describe example applications which enable intuitive and continuous zooming and scrolling. By using tiny thumb gesture input along with this pressure sensing method, bi-directional operations (e.g., zoom-in and -out) are also achieved.


% \textbf{
% Characteristics of pressure-based input for mobile devices  \cite{Stewart:2010:CPI:1753326.1753444}}

% We conducted a series of user studies to understand and clarify the fundamental characteristics of pressure in user interfaces for mobile devices. We seek to provide insight to clarify a longstanding discussion on mapping functions for pressure input. Previous literature is conflicted about the correct transfer function to optimize user performance. Our study results suggest that the discrepancy can be explained by different signal conditioning circuitry and with improved signal conditioning the user-performed precision relationship is linear. We also explore the effects of hand pose when applying pressure to a mobile device from the front, the back, or simultaneously from both sides in a pinching movement. Our results indicate that grasping type input outperforms single-sided input and is competitive with pressure input against solid surfaces. Finally we provide an initial exploration of non-visual multimodal feedback, motivated by the desire for eyes-free use of mobile devices. The findings suggest that non-visual pressure input can be executed without degradation in selection time but suffers from accuracy problems.


% \textbf{ PressureText: pressure input for mobile phone text entry \cite{McCallum:2009:PPI:1520340.1520693}}
% Pressure sensitive input has been predominantly used for text entry

% Pressure sensitive buttons are appealing for reducing repetitive tasks such as text entry on mobile phone keypads, where multiple key presses are currently necessary to record an action. We present PressureText, a text-entry technique for a pressure augmented mobile phone. In a study comparing PressureText to MultiTap, we found that despite limited visual feedfback for pressure input, users overall performed equally well with PressureText as with MultiTap. Expertise was a determining factor for improved performance with PressureText. Expert users showed a 33.6\% performance gain over novices. Additionally, expert users were 5\% faster on average with PressureText than MultiTap, suggesting that pressure input is a valuable augmentation to mobile phone keypads.

% \textbf{Exploring Continuous Pressure Input for Mobile Phones}

% % Reference : Clarkson, E.C., Patel, S., Pierce, J., and Abowd, G.D. Explor- ing Continuous Pressure Input for Mobile Phones. GVU Techreport; GIT-GVU-06-20, Georgia Inst. of Tech. (2006).

% The input capabilities of mobile phones are limited by their physical form factor. Approaches to augmenting those capabilities that expand the input space without negatively impacting size or weight are particularly desirable. We propose adding simple pressure sensors under the keypad buttons to provide multiple channels of continuous pressure input. Pressure input supports a larger and more interesting interaction space without some of the unusual or unwanted qualities of some other approaches. We describe an implementation of our pressure-augmented system and show a number of interaction techniques, some old and some new, facilitated by continual pressure. We contrast these techniques with previous sensor-augmentation devices and highlight notable differences and advantages.


% \textbf{ 
% Pressure-based text entry for mobile devices  \cite{Brewster:2009:PTE:1613858.1613870}}

% This paper describes the design and evaluation of a touch screen-based pressure keyboard to investigate the possibilities of pressure as a new method of input for mobile devices. A soft press on the touchscreen generated a lowercase letter, a hard press an uppercase one. The aim was to improve input performance when entering mixed-case text, or shifted characters often used for emoticons, etc. An experiment compared two different forms of pressure input (Dwell and Quick Release) against a standard shift key keyboard, with users both sitting and walking. Results showed that Quick Release was the fastest for input of mixed case text with Dwell being the most accurate, even when users were mobile. The results demonstrate that pressure input can outperform a standard shift-key keyboard design for mobile text entry.


% \textbf{
% PseudoButton: enabling pressure-sensitive interaction by repurposing microphone on mobile device
% \cite{Hwang:2012:PEP:2212776.2223673}}

% We propose a new interaction technique, called PseudoButton, which emulates a pressure-sensitive touch sensor by repurposing a built-in microphone on mobile devices. This simple and novel technique increases input expressivity of the device and expands its interaction area for users to alleviate the occlusion problem caused by touchscreens without adding extra sensors. To verify our idea, we implemented a prototype and conducted a preliminary evaluation on it. The results show that participants can input at accuracy of 94\% for five different pressure levels with minimal error.


% \textbf{MicPen: pressure-sensitive pen interaction using microphone with standard touchscreen \cite{Hwang:2012:MPP:2212776.2223717}}

% This paper introduces MicPen, a low-cost pressure-sensitive stylus pen interface for standard touchscreen displays that uses a microphone to estimate the amount of pressure applied to the pen. This is achieved by filtering and analyzing the acoustic signal generated when the tip of the pen is rubbed on the touchscreen. The advantage of this approach is that it is inexpensive, reliable and suitable for mobile interaction because it does not require mechanical parts to sense the input pressure. Results from a user study shows that the participants recognized five out of ten different pressure levels with perfect accuracy, and nine out of ten with minimal error.

% \textbf{
% GripSense: using built-in sensors to detect hand posture and pressure on commodity mobile phones
% \cite{Goel:2012:GUB:2380116.2380184}}

% We introduce GripSense, a system that leverages mobile device touchscreens and their built-in inertial sensors and vibration motor to infer hand postures including one- or two-handed interaction, use of thumb or index finger, or use on a table. GripSense also senses the amount of pres-sure a user exerts on the touchscreen despite a lack of direct pressure sensors by inferring from gyroscope readings when the vibration motor is "pulsed." In a controlled study with 10 participants, GripSense accurately differentiated device usage on a table vs. in hand with 99.67\% accuracy and when in hand, it inferred hand postures with 84.26\% accuracy. In addition, GripSense distinguished three levels of pressure with 95.1\% accuracy. A usability analysis of GripSense was conducted in three custom applications and showed that pressure input and hand-posture sensing can be useful in a number of scenarios.

% \textbf{Forcetap: extending the input vocabulary of mobile touch screens by adding tap gestures  \cite{Heo:2011:FEI:2037373.2037393}}

% We introduce an interaction technique that increases the touch screen input vocabulary by distinguishing a strong tap from a gentle tap without the use of additional hardware. We have designed and validated an algorithm that detects different types of screen touches by combining data from the built-in accelerometer with position data from the touch screen. The proposed technique allows a touch screen input to contain not only the position of a finger contact, but also its type, i.e., whether the contact is a 'Tap' or a 'ForceTap.' To verify the feasibility of the proposed technique we have implemented our detection algorithm in experiments that test cases of single-handed, two-handed, immersive, and on the move usage. Based on the experimental results, we investigate the advantages of using two types of touch inputs and discuss emerging issues. Finally, we suggest a design guideline for applying the proposed technique to touch screen applications, and present possible application scenarios.






% \section{B ContactShape}


% \textbf{ 
% Low-cost multi-touch sensing through frustrated total internal reflection \cite{Han:2005:LMS:1095034.1095054}}

% This paper describes a simple, inexpensive, and scalable technique for enabling high-resolution multi-touch sensing on rear-projected interactive surfaces based on frustrated total internal reflection. We review previous applications of this phenomenon to sensing, provide implementation details, discuss results from our initial prototype, and outline future directions.

% \textbf{ A multi-touch three dimensional touch-sensitive tablet \cite{Lee:1985:MTD:1165385.317461}}

% A prototype touch-sensitive tablet is presented. The tablet's main innovation is that it is capable of sensing more than one point of contact at a time. In addition to being able to provide position coordinates, the tablet also gives a measure of degree of contact, independently for each point of contact. In order to enable multi-touch sensing, the tablet surface is divided into a grid of discrete points. The points are scanned using a recursive area subdivision algorithm. In order to minimize the resolution lost due to the discrete nature of the grid, a novel interpolation scheme has been developed. Finally, the paper briefly discusses how multi-touch sensing, interpolation, and degree of contact sensing can be combined to expand our vocabulary in human-computer interaction.

% \textbf{
% Precise selection techniques for multi-touch screens \cite{Benko:2006:PST:1124772.1124963}}

% The size of human fingers and the lack of sensing precision can make precise touch screen interactions difficult. We present a set of five techniques, called Dual Finger Selections, which leverage the recent development of multi-touch sensitive displays to help users select very small targets. These techniques facilitate pixel-accurate targeting by adjusting the control-display ratio with a secondary finger while the primary finger controls the movement of the cursor. We also contribute a "clicking" technique, called SimPress, which reduces motion errors during clicking and allows us to simulate a hover state on devices unable to sense proximity. We implemented our techniques on a multi-touch tabletop prototype that offers computer vision-based tracking. In our formal user study, we tested the performance of our three most promising techniques (Stretch, X-Menu, and Slider) against our baseline (Offset), on four target sizes and three input noise levels. All three chosen techniques outperformed the control technique in terms of error rate reduction and were preferred by our participants, with Stretch being the overall performance and preference winner.

% \textbf{ 
% Navigating documents with the virtual scroll ring \cite{Moscovich:2004:NDV:1029632.1029642}}

% We present a technique for scrolling through documents that is simple to implement and requires no special hardware. This is accomplished by simulating a hardware scroll ring--a device that maps circular finger motion into vertical scrolling. The technique performs at least as well as a mouse wheel for medium and long distances, and is preferred by users. It can be particularly useful in portable devices where screen-space and space for peripherals is at a premium.


% \textbf{ ShapeTouch: Leveraging Contact Shape on Inter- active Surfaces.}

% % Cao, X., Wilson, A., Balakrishnan, R., Hinckley, K., and Hudson, S. ShapeTouch: Leveraging Contact Shape on Inter- active Surfaces. Proc. TABLETOP, IEEE (2008).

% Many interactive surfaces have the ability to detect the shape of hands or objects placed on them. However, shape information is typically either condensed to individual contact points or categorized as discrete gestures. This does not leverage the full expressiveness of touch input, thus limits the actions users can perform in interactive applications. We present ShapeTouch, an exploration of interactions that directly utilize the contact shape on interactive surfaces to manipulations of objects and interactors. ShapeTouch infers virtual contact forces from contact regions and motion to enable interaction with virtual objects in ways that draw upon userspsila everyday experiences of interacting with real physical objects.

% \textbf{ 
% AnglePose: robust, precise capacitive touch tracking via 3d orientation estimation \cite{Rogers:2011:ARP:1978942.1979318}}

% We present a finger-tracking system for touch-based interaction which can track 3D finger angle in addition to position, using low-resolution conventional capacitive sensors, therefore compensating for the inaccuracy due to pose variation in conventional touch systems. Probabilistic inference about the pose of the finger is carried out in real-time using a particle filter; this results in an efficient and robust pose estimator which also gives appropriate uncertainty estimates. We show empirically that tracking the full pose of the finger results in greater accuracy in pointing tasks with small targets than competitive techniques. Our model can detect and cope with different finger sizes and the use of either fingers or thumbs, bringing a significant potential for improvement in one-handed interaction with touch devices. In addition to the gain in accuracy we also give examples of how this technique could open up the space of novel interactions.


% \textbf{
% MicroRolls: expanding touch-screen input vocabulary by distinguishing rolls vs. slides of the thumb  \cite{Roudaut:2009:MET:1518701.1518843}}

% The input vocabulary for touch-screen interaction on handhelds is dramatically limited, especially when the thumb must be used. To enrich that vocabulary we propose to discriminate, among thumb gestures, those we call MicroRolls, characterized by zero tangential velocity of the skin relative to the screen surface. Combining four categories of thumb gestures, Drags, Swipes, Rubbings and MicroRolls, with other classification dimensions, we show that at least 16 elemental gestures can be automatically recognized. We also report the results of two experiments showing that the roll vs. slide distinction facilitates thumb input in a realistic copy and paste task, relative to existing interaction techniques.


% \textbf{ 
% Understanding touch \cite{Holz:2011:UT:1978942.1979308}}

% Current touch devices, such as capacitive touchscreens are based on the implicit assumption that users acquire targets with the center of the contact area between finger and device. Findings from our previous work indicate, however, that such devices are subject to systematic error offsets. This suggests that the underlying assumption is most likely wrong. In this paper, we therefore revisit this assumption. In a series of three user studies, we find evidence that the features that users align with the target are visual features. These features are located on the top of the user's fingers, not at the bottom, as assumed by traditional devices. We present the projected center model, under which error offsets drop to 1.6mm, compared to 4mm for the traditional model. This suggests that the new model is indeed a good approximation of how users conceptualize touch input. The primary contribution of this paper is to help understand touch-one of the key input technologies in human-computer interaction. At the same time, our findings inform the design of future touch input technology. They explain the inaccuracy of traditional touch devices as a -Sparallax- artifact between user control based on the top of the finger and sensing based on the bottom side of the finger. We conclude that certain camera-based sensing technologies can inherently be more accurate than contact area-based sensing.


% \textbf{ 
% Detecting and leveraging finger orientation for interaction with direct-touch surfaces \cite{Wang:2009:DLF:1622176.1622182}}

% Current interactions on direct-touch interactive surfaces are often modeled based on properties of the input channel that are common in traditional graphical user interfaces (GUI) such as x-y coordinate information. Leveraging additional information available on the surfaces could potentially result in richer and novel interactions. In this paper we specifically explore the role of finger orientation. This property is typically ignored in touch-based interactions partly because of the ambiguity in determining it solely from the contact shape. We present a simple algorithm that unambiguously detects the directed finger orientation vector in real-time from contact information only, by considering the dynamics of the finger landing process. Results of an experimental evaluation show that our algorithm is stable and accurate. We then demonstrate how finger orientation can be leveraged to enable novel interactions and to infer higher-level information such as hand occlusion or user position. We present a set of orientation-aware interaction techniques and widgets for direct-touch surfaces.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{On   using   contact   size   as   an   input   parameter   for   touch   screen
interaction}

Relevant work on simulated pressure techniques.  Contact size as an extra parameter
for input. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discuss what happens in both tablets and phones}

Here we might have papers that address both mobile phones and tablets or anything
that does not fit on the aforementioned categories.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ergonomically issues}

Possibly none since we use our index finger.
Fat finger should be compared and also distinguished form all those papers. Where it
differs and why it is unique and pioneer.